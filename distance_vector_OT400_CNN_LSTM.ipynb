{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "distance_vector_OT400_CNN_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54SHJwWl_RJ4"
      },
      "source": [
        "### Packages and global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkVbro84_XwM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "03556bc6-960a-4c44-ca4f-58c81b0ae7a1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "# Entering in the working directory \n",
        "%cd /content/drive/My\\ Drive/LCP_modB/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/LCP_modB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFbDApXg_RJ9"
      },
      "source": [
        "########################### Packages\n",
        "import sys\n",
        "import math \n",
        "import random\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
        "from PIL import Image\n",
        "\n",
        "########################### Global Variables\n",
        "data_path = 'work_dir/N400_ordered/'\n",
        "\n",
        "\n",
        "########################### Functions\n",
        "def seq_to_coords(data):\n",
        "    '''Function which returns a tuple made as follow:\n",
        "    (fcx,fcy,fcz,X,Y,Z) starting from the seqence (list)\n",
        "    called \"data\".\n",
        "    Explanation:\n",
        "        fcx,fcy,fcz are lists of the coordinates of all\n",
        "        of the sequence\n",
        "        X,Y,Z are lists of coordinates of vetices only  \n",
        "    '''\n",
        "    \n",
        "    cx, cy, cz = 0, 0, 0\n",
        "    fcx,fcy,fcz = [cx],[cy],[cz]\n",
        "    X,Y,Z = [cx],[cy],[cz]\n",
        "\n",
        "    N = len(data)\n",
        "\n",
        "    for i in range(0,N):\n",
        "        if data[i]=='0':\n",
        "            cx+=1\n",
        "        elif data[i]=='1':\n",
        "            cy+=1\n",
        "        elif data[i]=='2':\n",
        "            cz+=1        \n",
        "        elif data[i]=='3':\n",
        "            cx-=1\n",
        "        elif data[i]=='4':\n",
        "            cy-=1\n",
        "        elif data[i]=='5':\n",
        "            cz-=1\n",
        "\n",
        "        fcx.append(cx)\n",
        "        fcy.append(cy)\n",
        "        fcz.append(cz)\n",
        "\n",
        "        if i < (N-1) and data[i] != data[i+1]:\n",
        "            X.append(cx)\n",
        "            Y.append(cy)\n",
        "            Z.append(cz)\n",
        "\n",
        "        # Only vertices\n",
        "        # Even if the last point is NOT a vertex\n",
        "        # nevertheless it is considered such\n",
        "        if i == (N-1):\n",
        "            X.append(cx)\n",
        "            Y.append(cy)\n",
        "            Z.append(cz)\n",
        "\n",
        "    # Making all coordinates positive\n",
        "    minx,miny,minz=np.amin(fcx),np.amin(fcy),np.amin(fcz)\n",
        "    fcx = np.array(fcx)\n",
        "    fcy = np.array(fcy)\n",
        "    fcz = np.array(fcz)\n",
        "    fcx += abs(minx)\n",
        "    fcy += abs(miny)\n",
        "    fcz += abs(minz)\n",
        "\n",
        "    minX,minY,minZ=np.amin(X),np.amin(Y),np.amin(Z)\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "    Z = np.array(Z)\n",
        "    X += abs(minX)\n",
        "    Y += abs(minY)\n",
        "    Z += abs(minZ)\n",
        "    \n",
        "    # Understanding which between the first or \n",
        "    # the last point in (X,Y,Z) is a vertex\n",
        "    \n",
        "    # first is NOT a vertex\n",
        "    if (abs(np.sign(X[1]-X[-1])) \n",
        "        + abs(np.sign(Y[1]-Y[-1])) \n",
        "        + abs(np.sign(Z[1]-Z[-1]))) == 1:\n",
        "        X = X[1:]\n",
        "        Y = Y[1:]\n",
        "        Z = Z[1:]\n",
        "\n",
        "    # last is NOT a vertex\n",
        "    if (abs(np.sign(X[-2]-X[0])) \n",
        "        + abs(np.sign(Y[-2]-Y[0])) \n",
        "        + abs(np.sign(Z[-2]-Z[0])))==1:\n",
        "        X = X[:-1]\n",
        "        Y = Y[:-1]\n",
        "        Z = Z[:-1]\n",
        "        \n",
        "    return (fcx,fcy,fcz,X,Y,Z)\n",
        "    \n",
        "\n",
        "def dihedral(p1,p2,p3,X,Y,Z):\n",
        "    '''Function which computes generalized dihedral angle\n",
        "    This is a vectorize funzion, which computes the dihedral\n",
        "    angle with respect to a vector points: X, Y, Z\n",
        "    '''\n",
        "    \n",
        "    # ------ q1\n",
        "    q1 = p2 - p1 # v_(i-1)\n",
        "    \n",
        "    # ------ q2\n",
        "    q2 = p3 - p2 # v_i\n",
        "    q2_hat = q2/np.sqrt(np.dot(q2,q2))\n",
        "    \n",
        "    # ------ q3\n",
        "    # x = [1,2,3]\n",
        "    # roll(x,-1) -> [2,3,1]\n",
        "    tx = np.roll(X,-1) - X\n",
        "    ty = np.roll(Y,-1) - Y\n",
        "    tz = np.roll(Z,-1) - Z\n",
        "    \n",
        "    n = len(tx)\n",
        "    q3 = np.hstack((tx.reshape(n,1),\n",
        "                    ty.reshape(n,1),\n",
        "                    tz.reshape(n,1)))\n",
        "    \n",
        "    # norma per ogni riga e reshape per farlo diventare\n",
        "    # un vettore colonna (serve per broadcasting)\n",
        "    norm = np.linalg.norm(q3,axis=1).reshape(n,1)\n",
        "    q3_hat = q3/norm\n",
        "    # ------- /q3\n",
        "    \n",
        "    q1_x_q2 = np.cross(q1,q2)\n",
        "    n1 = q1_x_q2/np.sqrt(np.dot(q1_x_q2,q1_x_q2))\n",
        "    \n",
        "    # Qui n2 e u2 hanno la stessa forma di q3 (e q3_hat)\n",
        "    n2 = np.cross(q2_hat,q3_hat) # broadcasting di q2_hat per ogni riga di q3!\n",
        "    u2 = np.cross(q2_hat,n2)\n",
        "    \n",
        "    # con dot non funziona il broadcasting :C\n",
        "    cos_theta = np.array([n1.dot(v) for v in n2]) # vettore riga!!\n",
        "    sin_theta = np.array([n1.dot(v) for v in u2])\n",
        "    \n",
        "    return -np.arctan2(sin_theta,cos_theta)\n",
        "\n",
        "\n",
        "def edist(x1,y1,z1,X,Y,Z):\n",
        "    '''Returns a vector of distances from x1, y1, z1\n",
        "    '''\n",
        "    tx = (x1-X)**2\n",
        "    ty = (y1-Y)**2\n",
        "    tz = (z1-Z)**2\n",
        "    return np.sqrt(tx + ty + tz)\n",
        "\n",
        "\n",
        "def matrix_pad(m, filter_shape):\n",
        "    '''\n",
        "    This function returns the input matrix m with \"periodic padding\".\n",
        "        NB: The matrix and the filter are assumed to be 2 dimensional, and can be also rectangular matrices.\n",
        "            The assumption of m and filter being square matrices could be considered to speed the code by declaring their dimensions insted of using np.shape (maybe)\n",
        "    filter_shape is the shape of the mask; it can be a tuple or an integer if the mask is a square matrix.    \n",
        "    '''\n",
        "    \n",
        "    # create a tuple from the filter shape if filter_shape is an integer (so square mask, following the \"create_model()\" function notation)\n",
        "    if isinstance(filter_shape, int):\n",
        "        filter_shape = tuple([filter_shape]*len(m.shape))\n",
        "    \n",
        "    # shape of the padded matrix and initialization\n",
        "    new_shape = tuple([sum(x) for x in zip(m.shape, filter_shape)])\n",
        "    m_pad = np.zeros(shape = new_shape)\n",
        "    \n",
        "    # filling the padding matrix. the 2-dimensionality of m is assumed here; may be generalized.\n",
        "    m_pad[:m.shape[0], :m.shape[1]] = m  # filling with the original matrix\n",
        "    m_pad[m.shape[0]:, :m.shape[1]] = m[:filter_shape[0], :] # filling the rows under m with the first L rows of m\n",
        "    m_pad[:m.shape[0], m.shape[1]:] = m[:, :filter_shape[1]] # filling the columns at the right of m with the first L columns of m\n",
        "    m_pad[m.shape[0]:, m.shape[1]:] = m[:filter_shape[0], :filter_shape[1]] # filling the remained empty square in the diagonal\n",
        "    \n",
        "    return m_pad\n",
        "\n",
        "def to_binary(label):\n",
        "    '''Return the label vector as:\n",
        "    Unknotted -> 0\n",
        "    knotted -> 1\n",
        "    '''\n",
        "    \n",
        "    mask_unk = (label == 'UN')\n",
        "    mask_kn = (label != 'UN')\n",
        "    label[mask_unk] = 0\n",
        "    label[mask_kn] = 1\n",
        "    \n",
        "    return label\n",
        "  \n",
        "def e_dist_seq(X,Y,Z, n_step):\n",
        "  tx = (np.subtract(X,np.roll(X, n_step)))**2\n",
        "  ty = (np.subtract(Y,np.roll(Y, n_step)))**2\n",
        "  tz = (np.subtract(Z,np.roll(Z, n_step)))**2\n",
        "  return np.sqrt(tx + ty + tz)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TSPYVdC_RKQ"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1aVx_iy_RKT"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation, TimeDistributed\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, LSTM, Bidirectional, GlobalMaxPooling1D\n",
        "from tensorflow.keras.losses import categorical_crossentropy, binary_crossentropy\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "from tensorflow.keras import initializers, optimizers, regularizers, activations\n",
        "import gc # garbage collector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEubpYIh_RKj"
      },
      "source": [
        "### CNN topology"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnMfzznB_RKn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "f51b3f00-fca7-41b1-df84-e439dacb8282"
      },
      "source": [
        "def create_model(input_shape = 400, # rows, cols, channels, NOTE: is the input shape BEFORE the periodic padding\n",
        "                 init = initializers.he_normal(), # using He inizialization\n",
        "                 n_class=1,               # number of neuron last layer (classes)\n",
        "                 summary=False            # verbose\n",
        "                ):\n",
        "  \n",
        "    filter_shape = 100 # filter shape for the first Conv Layer\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(TimeDistributed(Conv1D(64,\n",
        "                 filter_shape,\n",
        "                 padding='valid',\n",
        "                 activation='relu'),\n",
        "                 input_shape = (40, input_shape + filter_shape, 1)))\n",
        "    \n",
        "    model.add(TimeDistributed(MaxPooling1D(pool_size=2, strides=4)))\n",
        "\n",
        "    model.add(TimeDistributed(Conv1D(64,\n",
        "                 50,\n",
        "                 padding='valid',\n",
        "                 activation='relu')))\n",
        "    \n",
        "    model.add(TimeDistributed(Conv1D(64,\n",
        "                 10,\n",
        "                 padding='valid',\n",
        "                 activation='relu')))\n",
        "    \n",
        "    model.add(Dropout(.3))\n",
        "    \n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "    model.add(Bidirectional(LSTM(10)))\n",
        "    \n",
        "    model.add(Dropout(.7))\n",
        "    \n",
        "    model.add(Dense(n_class, activation='sigmoid')) \n",
        "    \n",
        "        # print model's details\n",
        "    if summary: print(model.summary())\n",
        "    \n",
        "    # Compiling model\n",
        "    model.compile(loss = binary_crossentropy,\n",
        "                  optimizer = optimizers.Nadam(learning_rate = 0.0001),\n",
        "                  metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "    \n",
        "########### Defining the model\n",
        "model = create_model(summary=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed (TimeDistri (None, 40, 401, 64)       6464      \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 40, 100, 64)       0         \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 40, 51, 64)        204864    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 40, 42, 64)        41024     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 40, 42, 64)        0         \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 40, 2688)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 20)                215920    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 468,293\n",
            "Trainable params: 468,293\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zENjEKvx_RLI"
      },
      "source": [
        "### Reading files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiRXM4X-_RLK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "6572a226-fc2e-4e4f-a2b9-39b0d2cadbfe"
      },
      "source": [
        "files = sorted(listdir(data_path))\n",
        "files_classified = [s for s in files if s.endswith('.dat')]\n",
        "files_ent_classified = [s for s in files if s.endswith('dat.ent')]\n",
        "\n",
        "print('# files before manipulation:\\n',f'.dat = {len(files_classified)}', f'\\n.dat.ent = {len(files_ent_classified)}\\n')\n",
        "\n",
        "# removing corrupted date\n",
        "files_classified.remove('list10000polyg_N200_seq0023_be0.400_3d_ooo.dat')\n",
        "files_classified.remove(files_classified[0])\n",
        "files_ent_classified.remove(files_ent_classified[0])\n",
        "\n",
        "print('# files after manipulation:\\n',f'.dat = {len(files_classified)}', f'\\n.dat.ent = {len(files_ent_classified)}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# files before manipulation:\n",
            " .dat = 44 \n",
            ".dat.ent = 44\n",
            "\n",
            "# files after manipulation:\n",
            " .dat = 44 \n",
            ".dat.ent = 44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-2V7_-j_RLf"
      },
      "source": [
        "### Loading data in arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hghuoj_z_RLh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6508825-05cf-4082-cc5e-f97b53738928"
      },
      "source": [
        "dataset = [] \n",
        "labelset = []\n",
        "n1=[]\n",
        "n2=[]\n",
        "for file in files_classified:\n",
        "    #\n",
        "    print(f'\\n******** file: {file} *********\\n')\n",
        "    \n",
        "    ######### loading all the files\n",
        "    list1 = file # Choosing the i-file\n",
        "    list1_ent = file.replace('ooo.dat','hhh.dat.ent')\n",
        "\n",
        "    # Loading sequences from .dat\n",
        "    \n",
        "   \n",
        "    with open(data_path + list1, mode='r') as f:\n",
        "\n",
        "        data = np.array(f.readlines())\n",
        "        N_position = list(range(3,len(data)+1,4)) # position in which there are the sequences\n",
        "\n",
        "        # CHECK: saving 3rd number to compare it with the last\n",
        "        # column of the first entry in the .ent file\n",
        "        check_data = data[2].strip('\\n')\n",
        "\n",
        "        data = data[N_position]  # taking only sequences\n",
        "\n",
        "        # removing \"\\n\" at the end of each sequences\n",
        "        for idx,d in enumerate(data):\n",
        "            data[idx] = d.strip('\\n')\n",
        "    n1.append(len(data))\n",
        "    \n",
        "    # Loading labels from .dat.ent\n",
        "\n",
        "    with open(data_path + list1_ent, mode='r') as f:\n",
        "\n",
        "        label = []\n",
        "        for idx, line in enumerate(f):\n",
        "\n",
        "            # CHECK\n",
        "            if idx == 0:\n",
        "                check_label = line.split()[-1]\n",
        "\n",
        "            # saving labels (conteined in the 3rd column)\n",
        "            label.append(line.split()[2])\n",
        "\n",
        "            \n",
        "    n2.append(len(label))\n",
        "    \n",
        "    # changing labels and forcing to be floats\n",
        "    label = np.array(label)\n",
        "    #label = (to_binary(label)).astype('float')\n",
        "    # print(label)\n",
        "    print(f'------ CHECK .dat and .ent: {check_data==check_label} ------')\n",
        "    print(np.shape(data), np.shape(label))\n",
        "    t=data[:n2[-1]]\n",
        "    print(np.shape(dataset[:n2[-1]]))\n",
        "    dataset=np.concatenate((np.array(dataset), t))\n",
        "    labelset=np.concatenate((np.array(labelset),label))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "******** file: list10000polyg_N400_seq0001_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(0,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0002_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9995,)\n",
            "(9995,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0003_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9995,)\n",
            "(9995,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0004_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9997,)\n",
            "(9997,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0005_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9998,)\n",
            "(9998,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0006_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9998,)\n",
            "(9998,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0007_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9999,)\n",
            "(9999,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0008_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9998,)\n",
            "(9998,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0009_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(10000,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0010_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(10000,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0011_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(10000,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0012_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(10000,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0013_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9999,)\n",
            "(9999,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0014_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(10000,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0015_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9992,)\n",
            "(9992,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0016_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(10000,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0017_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9994,)\n",
            "(9994,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0018_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(10000,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0019_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9996,)\n",
            "(9996,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0020_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9986,)\n",
            "(9986,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0021_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9996,)\n",
            "(9996,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0022_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9995,)\n",
            "(9995,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0023_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9999,)\n",
            "(9999,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0024_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(10000,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0025_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9994,)\n",
            "(9994,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0026_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9998,)\n",
            "(9998,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0027_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9999,)\n",
            "(9999,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0028_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9998,)\n",
            "(9998,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0029_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9998,)\n",
            "(9998,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0030_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(10000,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0031_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9999,)\n",
            "(9999,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0032_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(10000,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0033_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9999,)\n",
            "(9999,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0034_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(10000,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0035_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(10000,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0036_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9997,)\n",
            "(9997,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0037_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9998,)\n",
            "(9998,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0038_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9997,)\n",
            "(9997,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0040_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9998,)\n",
            "(9998,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0041_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9999,)\n",
            "(9999,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0042_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9997,)\n",
            "(9997,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0043_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(10000,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0044_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (10000,)\n",
            "(10000,)\n",
            "\n",
            "******** file: list10000polyg_N400_seq0045_be0.400_3d_ooo.dat *********\n",
            "\n",
            "------ CHECK .dat and .ent: True ------\n",
            "(10000,) (9994,)\n",
            "(9994,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUbTxSRp_RLv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7568f170-c415-4f76-c75d-6be0575ea489"
      },
      "source": [
        "dataset = np.array(dataset).flatten()\n",
        "labelset = np.array(labelset).flatten()\n",
        "labelset = to_binary(labelset).astype('float')\n",
        "\n",
        "print('dataset shape:',dataset.shape,'labelset shape:',labelset.shape,'\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset shape: (439902,) labelset shape: (439902,) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbOvE363_RMC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "bb8880a4-4f2f-4a1b-9cbe-bca8bae16a12"
      },
      "source": [
        "# Shuffling in unison, it's the \"scary\" function from https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
        "\n",
        "def shuffle_in_unison(a, b):\n",
        "    rng_state = np.random.get_state()\n",
        "    np.random.shuffle(a)\n",
        "    np.random.set_state(rng_state)\n",
        "    np.random.shuffle(b)\n",
        "\n",
        "#simple example:\n",
        "#a = np.array([1,2,3,4])\n",
        "#b = np.array([5,6,7,8])\n",
        "#shuffle_in_unison(a,b)\n",
        "#print(a,b)\n",
        "\n",
        "shuffle_in_unison(dataset, labelset)\n",
        "\n",
        "print('Shuffled dataset and labels')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shuffled dataset and labels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLfoBI6q_RMM"
      },
      "source": [
        "### Selecting a chosen number of data to solve imbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ46qD6v_RMN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "e3346312-2d74-4da2-925e-cb6d805e8485"
      },
      "source": [
        "########################### Number of data per class\n",
        "batch_data = 20000\n",
        "###########################\n",
        "dataset=dataset[:len(labelset)]\n",
        "# Total dataset = batch_data*2\n",
        "knot_dataset = dataset[labelset==1]\n",
        "knot_labelset = labelset[labelset==1]\n",
        "unknot_dataset = dataset[labelset==0]\n",
        "unknot_labelset = labelset[labelset==0]\n",
        "\n",
        "# selecting knots\n",
        "index_knot = np.random.choice(knot_dataset.shape[0], batch_data, replace=False)\n",
        "chosen_knot_dataset = knot_dataset[index_knot]\n",
        "chosen_knot_labelset = knot_labelset[index_knot]\n",
        "print(f'Selected {chosen_knot_dataset.shape} knots',f' with {chosen_knot_labelset.shape} labels')\n",
        "# selecting unknots\n",
        "index_unknot = np.random.choice(unknot_dataset.shape[0], batch_data, replace=False)\n",
        "chosen_unknot_dataset = unknot_dataset[index_unknot]\n",
        "chosen_unknot_labelset = unknot_labelset[index_unknot]\n",
        "print(f'Selected {chosen_unknot_dataset.shape} unknots',f' with {chosen_unknot_labelset.shape} labels')\n",
        "\n",
        "merged_dataset = np.concatenate((chosen_knot_dataset, chosen_unknot_dataset))\n",
        "merged_labelset = np.concatenate((chosen_knot_labelset, chosen_unknot_labelset))\n",
        "\n",
        "print(f'Dataset shape: {merged_dataset.shape}\\n ',f' Label shape: {merged_labelset.shape}')\n",
        "\n",
        "# Shuffling the data\n",
        "\n",
        "shuffle_in_unison(merged_dataset, merged_labelset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected (20000,) knots  with (20000,) labels\n",
            "Selected (20000,) unknots  with (20000,) labels\n",
            "Dataset shape: (40000,)\n",
            "   Label shape: (40000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbWeQpw4ltKI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "98888b4b-e7b0-4edc-c61f-25b1d788fbee"
      },
      "source": [
        "knot_dataset = dataset[labelset==1]\n",
        "knot_labelset = labelset[labelset==1]\n",
        "unknot_dataset = dataset[labelset==0]\n",
        "unknot_labelset = labelset[labelset==0]\n",
        "\n",
        "# IMPORTANTE: meglio uno shuffle prima di separare i dati\n",
        "shuffle_in_unison(knot_dataset, knot_labelset)\n",
        "shuffle_in_unison(unknot_dataset, unknot_labelset)\n",
        "print('Shuffled dataset and labels')\n",
        "\n",
        "# 50% knot 50% unknot\n",
        "chosen_unknot_dataset = unknot_dataset[:len(knot_dataset)]\n",
        "chosen_unknot_labelset = unknot_labelset[:len(knot_dataset)]\n",
        "\n",
        "# 42% knot 58% unknot\n",
        "# surplus = 1000\n",
        "# chosen_unknot_dataset = unknot_dataset[:len(knot_dataset)+surplus]\n",
        "# chosen_unknot_labelset = unknot_labelset[:len(knot_dataset)+surplus]\n",
        "\n",
        "# # 36% knot 64% unknot\n",
        "# surplus = 20000\n",
        "# chosen_unknot_dataset = unknot_dataset[:len(knot_dataset)+surplus]\n",
        "# chosen_unknot_labelset = unknot_labelset[:len(knot_dataset)+surplus]\n",
        "\n",
        "# NOTABENE: usando len(knot_dataset) anche per selezionare i chosen_unknot_dataset\n",
        "# faccio in modo che il dataset di test sia equilibrato, nell'avere sempre 50 \n",
        "# e 50 tra nodi e non nodi\n",
        "\n",
        "frac_test = 0.075\n",
        "M = int(frac_test*len(knot_dataset))\n",
        "print(f'M: {M}')\n",
        "X_test = np.concatenate((knot_dataset[:M], chosen_unknot_dataset[:M]))\n",
        "Y_test = np.concatenate((knot_labelset[:M], chosen_unknot_labelset[:M]))\n",
        "\n",
        "merged_dataset = np.concatenate((knot_dataset[M:], chosen_unknot_dataset[M:]))\n",
        "merged_labelset = np.concatenate((knot_labelset[M:], chosen_unknot_labelset[M:]))\n",
        "\n",
        "print(f'Original dataset shape: {knot_dataset.shape[0] + chosen_unknot_dataset.shape[0]}')\n",
        "print(f'Dataset shape: {merged_dataset.shape}\\nLabel shape: {merged_labelset.shape}')\n",
        "print(f'X_test shape: {X_test.shape}\\nY_label shape: {Y_test.shape}')\n",
        "\n",
        "shuffle_in_unison(merged_dataset, merged_labelset)\n",
        "shuffle_in_unison(X_test, Y_test)\n",
        "print('Merged and Test sets reshuffled')\n",
        "\n",
        "def dihedral(p1,p2,p3,p4):\n",
        "    q1 = np.subtract(p2,p1) # b - a\n",
        "    q2 = np.subtract(p3,p2) # c - b\n",
        "    q3 = np.subtract(p4,p3) # d - c\n",
        "    q1_x_q2 = np.cross(q1,q2)\n",
        "    q2_x_q3 = np.cross(q2,q3)\n",
        "    n1 = q1_x_q2/np.sqrt(np.dot(q1_x_q2,q1_x_q2))\n",
        "    n2 = q2_x_q3/np.sqrt(np.dot(q2_x_q3,q2_x_q3))\n",
        "    u1 = n2\n",
        "    u3 = q2/(np.sqrt(np.dot(q2,q2)))\n",
        "    u2 = np.cross(u3,u1)\n",
        "    cos_theta = np.dot(n1,u1)\n",
        "    sin_theta = np.dot(n1,u2)\n",
        "    theta = -math.atan2(sin_theta,cos_theta) \n",
        "    return theta\n",
        "\n",
        "def vector_pad(v, filter_shape):\n",
        "    \n",
        "    # shape of the padded matrix and initialization\n",
        "    new_shape = v.size + filter_shape\n",
        "    v_pad = np.zeros(new_shape)\n",
        "    \n",
        "    # filling the padding matrix. the 2-dimensionality of m is assumed here; may be generalized.\n",
        "    v_pad[:v.shape[0]] = v  # filling with the original matrix\n",
        "    v_pad[v.shape[0]:] = v[:filter_shape] # filling the rows under m with the first L rows of m\n",
        "        \n",
        "    return v_pad\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shuffled dataset and labels\n",
            "M: 6365\n",
            "Original dataset shape: 169736\n",
            "Dataset shape: (157006,)\n",
            "Label shape: (157006,)\n",
            "X_test shape: (12730,)\n",
            "Y_label shape: (12730,)\n",
            "Merged and Test sets reshuffled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4UPgn_Z_RMZ"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDWNwNGf_RMa"
      },
      "source": [
        "train_acc = [] # to save training accuracy during multiple training\n",
        "val_acc = [] # validation accuracy\n",
        "train_loss = [] \n",
        "val_loss = []\n",
        "\n",
        "print(f'merged_dataset shape: {merged_dataset.shape}')\n",
        "print(f'merged_labelset: {merged_labelset.shape}')\n",
        "\n",
        "N_data = int(len(merged_dataset)*0.3) # number of sequences\n",
        "N_data_test= int(len(merged_dataset)*0.01)\n",
        "len_seq = len(merged_dataset[0]) + 1 # number of digits per sequence\n",
        "\n",
        "print('** Starting converting to distances')\n",
        "\n",
        "channels=50\n",
        "\n",
        "filter_dim = model.layers[0].get_weights()[0].shape[0]\n",
        "X = np.zeros(shape = (N_data, len_seq + filter_dim, channels), dtype=np.float16)\n",
        "X_test = np.zeros(shape = (N_data_test, len_seq + filter_dim, channels), dtype=np.float16)\n",
        "\n",
        "\n",
        "for idx, seq in enumerate(merged_dataset[:N_data]):\n",
        "\n",
        "  fcx, fcy, fcz, _, _, _ = seq_to_coords(seq) # transcription of the sequence\n",
        "  for c in range(channels):\n",
        "    d = e_dist_seq(fcx, fcy, fcz, c+2)\n",
        "    d = (d - np.min(d))/( np.max(d) - np.min(d) ) # scaling\n",
        "    d = vector_pad(d, filter_dim) # padding\n",
        "    X[idx,:,c] = d\n",
        "# X=X.reshape(N_data ,len_seq + filter_dim, c)\n",
        "\n",
        "for idx, seq in enumerate(merged_dataset[N_data+1:N_data+N_data_test+1]):\n",
        "  for c in range(channels):\n",
        "\n",
        "    fcx, fcy, fcz, _, _, _ = seq_to_coords(seq) # transcription of the sequence\n",
        "    # for c in range(channels):\n",
        "    d = e_dist_seq(fcx, fcy, fcz, c+2)\n",
        "    d = (d - np.min(d))/( np.max(d) - np.min(d) ) # scaling\n",
        "    d = vector_pad(d, filter_dim) # padding\n",
        "    X_test[idx,:,c] = d\n",
        "\n",
        "\n",
        "print(f'** Process Completed for {N_data} sequences')\n",
        "\n",
        "Y = merged_labelset[:N_data]\n",
        "Y_test = merged_labelset[N_data+1:N_data_test+N_data+1]\n",
        "\n",
        "# ############ Parameters for the training\n",
        "epochs = 100\n",
        "step_update = 50    # number of samples before gradient update\n",
        "                    # It must be < batch_size\n",
        "validation_split = 0.15 # fraction of data used as validation\n",
        "                        # Keras do everything by itself\n",
        "\n",
        "fit = model.fit(X, Y,\n",
        "            batch_size = step_update,\n",
        "            epochs = epochs,\n",
        "            # validation_data = (X_val,Y_val),\n",
        "            validation_split = validation_split,\n",
        "            verbose = 1,\n",
        "            shuffle = True)\n",
        "\n",
        "# Saving statistics\n",
        "train_acc.append(fit.history['accuracy'])\n",
        "val_acc.append(fit.history['val_accuracy'])\n",
        "train_loss.append(fit.history['loss'])\n",
        "val_loss.append(fit.history['val_loss'])\n",
        "del X,Y\n",
        "gc.collect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2eVErFER4ll"
      },
      "source": [
        "model.save('model_400_distance_vectors_40channels') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uSPOHN9R4G_"
      },
      "source": [
        "# Y_test = merged_labelset[-N_data_test:]\n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRl1HDwwtp8l"
      },
      "source": [
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "\n",
        "N_data = len(X_test) # number of sequences\n",
        "len_seq = len(X_test[0]) + 1 # number of digits per sequence\n",
        "\n",
        "p = model.predict(X_test)\n",
        "m = BinaryAccuracy(name = 'binary_accuracy', \n",
        "                   threshold = 0.5)\n",
        "                   # if predict is < 0.5 then it's considered as 0\n",
        "                  \n",
        "m.update_state(Y_test, p) # feeding labels\n",
        "acc_test = m.result().numpy() # result\n",
        "print(acc_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix \n",
        "# round(): se <= .5 allora diventa 0 (come per la BinaryAccuracy)\n",
        "c_matrix = confusion_matrix(Y_test, p.round(), normalize='true',\n",
        "                           labels=[0,1])\n",
        "print(c_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ADh70Lo_RMz"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(20,8))\n",
        "    \n",
        "ax[0].plot(fit.history['accuracy'], '--', \n",
        "         label = 'train acc')\n",
        "\n",
        "ax[0].plot(fit.history['val_accuracy'], \n",
        "         label = 'val acc') \n",
        "\n",
        "fontsize = 14\n",
        "ax[0].set_ylabel('accuracy', fontsize=fontsize)\n",
        "ax[0].set_xlabel('Epochs', fontsize=fontsize)\n",
        "ax[0].legend(fontsize=fontsize)    \n",
        "    \n",
        "ax[1].plot(fit.history['loss'], '--', \n",
        "         label = 'training loss')\n",
        "\n",
        "ax[1].plot(fit.history['val_loss'], \n",
        "         label = 'validation loss') \n",
        "\n",
        "ax[1].set_ylabel('loss', fontsize=fontsize)\n",
        "ax[1].set_xlabel('Epochs', fontsize=fontsize)\n",
        "ax[1].legend(fontsize=fontsize)    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}